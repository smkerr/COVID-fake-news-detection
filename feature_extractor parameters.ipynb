{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature_extractor parameters.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPb1qynPwdTUwazhrxnQIS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Grid Search coupling parameters from a text documents feature extractor (n-gram count vectorizer and TF-IDF transformer) "],"metadata":{"id":"16U-KOO5fEpj"}},{"cell_type":"markdown","source":["Source: \n","https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"],"metadata":{"id":"h7b8gejce01Q"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","from nltk import sent_tokenize\n","from nltk.corpus import stopwords\n","stopwords = nltk.corpus.stopwords.words('english')\n","from nltk.tokenize import word_tokenize\n","\n","import string\n","import joblib\n","import re\n","# set seed\n","np.random.seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4UgNAvl1trw","executionInfo":{"status":"ok","timestamp":1649874037064,"user_tz":-120,"elapsed":2643,"user":{"displayName":"Marco Schildt","userId":"08365269881482043900"}},"outputId":"793bcee3-50a4-4b81-e517-7e6a182015f0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["train_df = pd.read_csv('https://raw.githubusercontent.com/smkerr/COVID-fake-news-detection/main/data/Constraint_Train.csv', header=0)\n","val_df = pd.read_csv('https://raw.githubusercontent.com/smkerr/COVID-fake-news-detection/main/data/Constraint_Val.csv', header=0)"],"metadata":{"id":"eccml98L02yE","executionInfo":{"status":"ok","timestamp":1649874038408,"user_tz":-120,"elapsed":426,"user":{"displayName":"Marco Schildt","userId":"08365269881482043900"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["stopwords = nltk.corpus.stopwords.words('english')\n","\n","def cleantext(string):\n","    text = string.lower().split()\n","    text = \" \".join(text)\n","    text = re.sub(r\"http(\\S)+\",' ',text)    \n","    text = re.sub(r\"www(\\S)+\",' ',text)\n","    text = re.sub(r\"&\",' and ',text)  \n","    tx = text.replace('&amp',' ')\n","    text = re.sub(r\"[^a-zA-Z]+\",' ',text)\n","    text = text.split()\n","    text = [w for w in text if not w in stopwords]\n","    text = \" \".join(text)\n","    return text"],"metadata":{"id":"2ge5FWht5M63","executionInfo":{"status":"ok","timestamp":1649874041531,"user_tz":-120,"elapsed":233,"user":{"displayName":"Marco Schildt","userId":"08365269881482043900"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train_df['tweet'] = train_df['tweet'].map(lambda x: cleantext(x))\n","val_df['tweet'] = val_df['tweet'].map(lambda x: cleantext(x))"],"metadata":{"id":"WcgQb0Nd5PpT","executionInfo":{"status":"ok","timestamp":1649874043711,"user_tz":-120,"elapsed":913,"user":{"displayName":"Marco Schildt","userId":"08365269881482043900"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from pprint import pprint\n","from time import time\n","import logging\n","\n","#from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","\n","# Display progress logs on stdout\n","#logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n","\n","pipeline = Pipeline(\n","    [\n","        (\"vect\", CountVectorizer()),\n","        (\"tfidf\", TfidfTransformer()),\n","        (\"clf\", SGDClassifier()),\n","    ]\n",")\n","\n","# uncommenting more parameters will give better exploring power but will\n","# increase processing time in a combinatorial way\n","parameters = {\n","    #\"vect__max_df\": (0.5, 0.75, 1.0),\n","    #'vect__max_features': (None, 5000, 10000, 50000),\n","    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n","    'tfidf__use_idf': (True, False), \n","    #'tfidf__norm': ('l1', 'l2'),\n","    #\"clf__max_iter\": (20,),\n","    #\"clf__alpha\": (0.00001, 0.000001),\n","    #\"clf__penalty\": (\"l2\", \"elasticnet\"),\n","    #'clf__max_iter': (10, 50, 80),\n","}\n","\n","if __name__ == \"__main__\":\n","    # multiprocessing requires the fork to happen in a __main__ protected\n","    # block\n","\n","    # find the best parameters for both the feature extraction and the\n","    # classifier\n","    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring=\"f1_micro\" )\n","\n","    print(\"Performing grid search...\")\n","    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n","    print(\"parameters:\")\n","    pprint(parameters)\n","    t0 = time()\n","    grid_search.fit(train_df['tweet'],train_df['label'])\n","    print(\"done in %0.3fs\" % (time() - t0))\n","    print()\n","\n","    print(\"Best score: %0.3f\" % grid_search.best_score_)\n","    print(\"Best parameters set:\")\n","    best_parameters = grid_search.best_estimator_.get_params()\n","    for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","\n","    grid_search.cv_results_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBGwnuogfqiE","executionInfo":{"status":"ok","timestamp":1649877213773,"user_tz":-120,"elapsed":6527,"user":{"displayName":"Marco Schildt","userId":"08365269881482043900"}},"outputId":"bf10447a-4e2b-4091-a08b-85eeac608cd0"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing grid search...\n","pipeline: ['vect', 'tfidf', 'clf']\n","parameters:\n","{'tfidf__use_idf': (True, False), 'vect__ngram_range': ((1, 1), (1, 2))}\n","Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","done in 6.237s\n","\n","Best score: 0.934\n","Best parameters set:\n","\ttfidf__use_idf: True\n","\tvect__ngram_range: (1, 2)\n"]}]},{"cell_type":"markdown","source":["### Results"],"metadata":{"id":"MT2GCJ_uW1t4"}},{"cell_type":"markdown","source":["The best parameters set is:\n","\n","*   tfidf__use_idf: True (Which is the default)\n","*   Lvect__ngram_range: (1, 2) (means unigrams and bigrams)"],"metadata":{"id":"h6T5o-PgWNAK"}}]}