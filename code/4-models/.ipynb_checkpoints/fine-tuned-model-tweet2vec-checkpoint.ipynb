{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X96VzyHuGU4-"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1813,
     "status": "ok",
     "timestamp": 1651521489938,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "LDUxonlU7VeH",
    "outputId": "a7107017-a6d0-4de1-dab2-60d9ac71726c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/steve/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/steve/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import spacy\n",
    "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0.tar.gz &> /dev/null\n",
    "nlp = spacy.load(\"en_core_web_md\") # md: reduced word vector table with 20k unique vectors for ~500k words\n",
    "\n",
    "# custom functions\n",
    "from functions import clean_text, print_metrics, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1651521490272,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "mQiCiSQUBO75"
   },
   "outputs": [],
   "source": [
    "# load & clean data \n",
    "train = pd.read_csv('https://raw.githubusercontent.com/smkerr/COVID-fake-news-detection/main/data/original-data/Constraint_Train.csv', header=0)\n",
    "train_clean = train[train[\"tweet\"].map(len) <= 280].drop_duplicates() # drop posts longer than 280 characters & drop duplicates\n",
    "X_train, y_train = train_clean[\"tweet\"], train_clean[\"label\"]\n",
    "\n",
    "\n",
    "val = pd.read_csv('https://raw.githubusercontent.com/smkerr/COVID-fake-news-detection/main/data/original-data/Constraint_Val.csv', header=0)\n",
    "val_clean = val[val[\"tweet\"].map(len) <= 280].drop_duplicates()  # drop posts longer than 280 characters & drop duplicates\n",
    "X_val, y_val = val_clean[\"tweet\"], val_clean[\"label\"]\n",
    "\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/smkerr/COVID-fake-news-detection/main/data/original-data/Constraint_Test.csv', header=0)\n",
    "test_clean = test[test[\"tweet\"].map(len) <= 280].drop_duplicates()  # drop posts longer than 280 characters & drop duplicates\n",
    "X_test, y_test = test_clean[\"tweet\"], test_clean[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1RHfeVkGYNg"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3445,
     "status": "ok",
     "timestamp": 1651521508236,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "LlXp-6juE_8t",
    "outputId": "f005bd32-c6c5-4e1e-97ef-816f60d6356f"
   },
   "outputs": [],
   "source": [
    "# apply clean_text() function to all tweets \n",
    "X_train = X_train.map(lambda x: clean_text(x))\n",
    "X_val = X_val.map(lambda x: clean_text(x))\n",
    "X_test = X_test.map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651521508236,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "W2nxpRa27d6i"
   },
   "outputs": [],
   "source": [
    "# initialize label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# encode 'fake' as 0 and 'real' as 1 to make target variables machine-readable\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.fit_transform(y_val)\n",
    "y_test = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8fAkUZbSKZb"
   },
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn9TbYAmMfWM"
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 78876,
     "status": "ok",
     "timestamp": 1651521589931,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "4Pds9s8jMfWN"
   },
   "outputs": [],
   "source": [
    "# generate word vectors for each tweet \n",
    "## train set \n",
    "tweet2vec_list = [nlp(doc).vector.reshape(1,-1) for doc in X_train] # creates 6420x1 list, with each entry containing a 300x1 np.array word vector corresponding with a tweet\n",
    "tweet2vec_data = np.concatenate(tweet2vec_list) # joins word vectors into 6420x300 np.array\n",
    "tweet2vec_train = pd.DataFrame(tweet2vec_data) # convert to data frame\n",
    "\n",
    "## validation set \n",
    "tweet2vec_list = [nlp(doc).vector.reshape(1,-1) for doc in X_val] # creates 6420x1 list, with each entry containing a 300x1 np.array word vector corresponding with a tweet\n",
    "tweet2vec_data = np.concatenate(tweet2vec_list) # joins word vectors into 6420x300 np.array\n",
    "tweet2vec_val = pd.DataFrame(tweet2vec_data) # convert to data frame\n",
    "\n",
    "## test set \n",
    "tweet2vec_list = [nlp(doc).vector.reshape(1,-1) for doc in X_test] # creates 6420x1 list, with each entry containing a 300x1 np.array word vector corresponding with a tweet\n",
    "tweet2vec_data = np.concatenate(tweet2vec_list) # joins word vectors into 6420x300 np.array\n",
    "tweet2vec_test = pd.DataFrame(tweet2vec_data) # convert to data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO_3oqJGXjnN"
   },
   "source": [
    "## Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1651516927107,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "xS1DYYRMSUz2"
   },
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "cv = CountVectorizer(ngram_range=(1, 2)) # count term frequency\n",
    "\n",
    "# fit and transform train data to count vectorizer\n",
    "cv.fit(X_train.values)\n",
    "cv_train = cv.transform(X_train.values)\n",
    "\n",
    "# fit and transform validation data to counter vectorizer\n",
    "cv_val = cv.transform(X_val.values)\n",
    "\n",
    "# fit and transform validation data to counter vectorizer\n",
    "cv_test = cv.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1651516930274,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "1YUXus_ioR5Y"
   },
   "outputs": [],
   "source": [
    "# rename word2vec columns to de-conflict merge\n",
    "\n",
    "## train set\n",
    "### create list of word embedding column names\n",
    "word2vec_col = []\n",
    "for i in range(len(tweet2vec_train.columns)):\n",
    "  num = str(i)\n",
    "  name = \"word2vec_\"+num\n",
    "  word2vec_col.append(name) \n",
    "\n",
    "### rename word embedding columns \n",
    "tweet2vec_train.columns = word2vec_col\n",
    "\n",
    "## validation set\n",
    "### create list of word embedding column names\n",
    "word2vec_col = []\n",
    "for i in range(len(tweet2vec_val.columns)):\n",
    "  num = str(i)\n",
    "  name = \"word2vec_\"+num\n",
    "  word2vec_col.append(name) \n",
    "\n",
    "### rename word embedding columns \n",
    "tweet2vec_val.columns = word2vec_col\n",
    "\n",
    "## test set\n",
    "### create list of word embedding column names\n",
    "word2vec_col = []\n",
    "for i in range(len(tweet2vec_test.columns)):\n",
    "  num = str(i)\n",
    "  name = \"word2vec_\"+num\n",
    "  word2vec_col.append(name) \n",
    "\n",
    "### rename word embedding columns \n",
    "tweet2vec_test.columns = word2vec_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yhIIAkXoh3"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1651516932541,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "XG3Q80cZXqXG"
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# fit the CountVector to TF-IDF transformer\n",
    "tfidf.fit(cv_train)\n",
    "tfidf_train = tfidf.transform(cv_train)\n",
    "\n",
    "# do the same for the validation set\n",
    "tfidf.fit(cv_val)\n",
    "tfidf_val = tfidf.transform(cv_val)\n",
    "\n",
    "# and the same for the validation set\n",
    "tfidf.fit(cv_test)\n",
    "tfidf_test = tfidf.transform(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1651516936350,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "LfcL820MoR5Y"
   },
   "outputs": [],
   "source": [
    "# convert tfidf_train to data frame\n",
    "## train set\n",
    "tfidf_train = pd.DataFrame(tfidf_train.toarray())\n",
    "\n",
    "## validation set \n",
    "tfidf_val = pd.DataFrame(tfidf_val.toarray())\n",
    "\n",
    "## test set \n",
    "tfidf_test = pd.DataFrame(tfidf_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1651516936351,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "Q3vDunFxoR5Z"
   },
   "outputs": [],
   "source": [
    "# rename tfidf columns to de-conflict merge\n",
    "\n",
    "## train set\n",
    "### create list of tfidf column names\n",
    "tfidf_col = []\n",
    "for i in range(len(tfidf_train.columns)):\n",
    "  num = str(i)\n",
    "  name = \"tfidf_\"+num\n",
    "  tfidf_col.append(name) \n",
    "\n",
    "### rename tfidf columns\n",
    "tfidf_train.columns = tfidf_col\n",
    "\n",
    "## validation set\n",
    "### create list of tfidf column names\n",
    "tfidf_col = []\n",
    "for i in range(len(tfidf_val.columns)):\n",
    "  num = str(i)\n",
    "  name = \"tfidf_\"+num\n",
    "  tfidf_col.append(name) \n",
    "\n",
    "### rename tfidf columns\n",
    "tfidf_val.columns = tfidf_col\n",
    "\n",
    "## test set\n",
    "### create list of tfidf column names\n",
    "tfidf_col = []\n",
    "for i in range(len(tfidf_test.columns)):\n",
    "  num = str(i)\n",
    "  name = \"tfidf_\"+num\n",
    "  tfidf_col.append(name) \n",
    "\n",
    "### rename tfidf columns\n",
    "tfidf_test.columns = tfidf_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ5OnihEoR5Y"
   },
   "source": [
    "## Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kP23ax2DoR5Z"
   },
   "outputs": [],
   "source": [
    "# join tf-idf with word embeddings \n",
    "\n",
    "## train set \n",
    "X_train = tfidf_train.join(tweet2vec_train) \n",
    "\n",
    "## validation set \n",
    "X_val = tfidf_val.join(tweet2vec_val) \n",
    "\n",
    "## test set \n",
    "X_test = tfidf_test.join(tweet2vec_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBGrHpbcBqEB"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickled classifiers from file\n",
    "#with open(\"pickle_svm_clf.pkl\", 'rb') as file:\n",
    "#    pickle_svm_clf = pickle.load(file)\n",
    "#\n",
    "#with open(\"pickle_lg_clf.pkl\", 'rb') as file:\n",
    "#    pickle_lr_clf = pickle.load(file)\n",
    "#    \n",
    "#with open(\"pickle_xgb_clf.pkl\", 'rb') as file:\n",
    "#    pickle_xgb_clf = pickle.load(file)\n",
    "#    \n",
    "#with open(\"pickle_ada_clf.pkl\", 'rb') as file:\n",
    "#    pickle_ada_clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XnE-4bmBrQk"
   },
   "source": [
    "## #1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#pickle_svm_pred_val = pickle_svm_clf.predict(X_val)\n",
    "#pickle_svm_pred_test = pickle_svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYf2PXd2TyCL",
    "outputId": "5030df76-0ea6-4627-a2fa-8ea37ab33f41"
   },
   "outputs": [],
   "source": [
    "# create SVC object \n",
    "svm_clf = SVC(kernel='linear',probability=True, C=10, class_weight='balanced')\n",
    "\n",
    "# create pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "        #('bow', CountVectorizer(ngram_range=(1, 2))), # count term frequency\n",
    "        #('tfidf', TfidfTransformer()), # downweight words which appear frequently\n",
    "        ('c', svm_clf) # classifier\n",
    "    ])\n",
    "\n",
    "# train model\n",
    "fit = svm_pipeline.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "svm_pred_val = svm_pipeline.predict(X_val)\n",
    "svm_pred_test = svm_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay0JYxPCB4sD"
   },
   "source": [
    "## #2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#pickle_lr_pred_val = pickle_lr_clf.predict(X_val)\n",
    "#pickle_lr_pred_test = pickle_lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZsimwCxO-du",
    "outputId": "b8cef749-bccd-47ac-bc16-2cf713b79e1d"
   },
   "outputs": [],
   "source": [
    "# create logistic regression object\n",
    "lr_clf = LogisticRegression(max_iter=1000, penalty='none', solver='saga')\n",
    "\n",
    "# create pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "        #('count', CountVectorizer(ngram_range=(1, 2))), # count term frequency\n",
    "        #('tfidf', TfidfTransformer()), # downweight words which appear frequently\n",
    "        ('c', lr_clf) # classifier\n",
    "    ])\n",
    "\n",
    "# train model\n",
    "fit = lr_pipeline.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "lr_pred_val = lr_pipeline.predict(X_val) # validation set \n",
    "lr_pred_test = lr_pipeline.predict(X_test) # test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcYMm6vZB7-V"
   },
   "source": [
    "## #3 Extreme Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#pickle_xgb_pred_val = pickle_xgb_clf.predict(X_val)\n",
    "#pickle_xgb_pred_test = pickle_xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "FeZ9DHPNpNNN",
    "outputId": "77bd2c23-6dff-4bc5-9e5b-a79511b27c59"
   },
   "outputs": [],
   "source": [
    "# create XGBoost object \n",
    "xgb_clf = XGBClassifier(max_depth=3, min_child_weight=3, eta = 0.15, n_estimators = 550, subsample=0.85)\n",
    "\n",
    "# create pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "        #('bow', CountVectorizer(ngram_range=(1, 1))), # count term frequency\n",
    "        #('tfidf', TfidfTransformer()), # downweight words which appear frequently\n",
    "        ('c', xgb_clf) # classifier\n",
    "])\n",
    " \n",
    "# train model \n",
    "fit = xgb_pipeline.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "xgb_pred_val = xgb_pipeline.predict(X_val) # validation set \n",
    "xgb_pred_test = xgb_pipeline.predict(X_test) # test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcYc9QrCB_9j"
   },
   "source": [
    "## #4 Adaptive Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#pickle_ada_pred_val = pickle_ada_clf.predict(X_val)\n",
    "#pickle_ada_pred_test = pickle_ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKs_IrTSBYQw"
   },
   "outputs": [],
   "source": [
    "# create AdaBoost object\n",
    "ada_clf = AdaBoostClassifier(n_estimators=500, learning_rate = 1.0)\n",
    "\n",
    "# create pipeline\n",
    "ada_pipeline = Pipeline([\n",
    "        #('bow', CountVectorizer(ngram_range=(1, 2))), # count term frequency\n",
    "        #('tfidf', TfidfTransformer()), # downweight words which appear frequently\n",
    "        ('c', ada_clf) # classifier\n",
    "])\n",
    "\n",
    "# train model\n",
    "fit = ada_pipeline.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "ada_pred_val = ada_pipeline.predict(X_val) # validation set\n",
    "ada_pred_test = ada_pipeline.predict(X_test) # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcdD5s2tM4mU"
   },
   "source": [
    "## #5 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#pickle_voting_pred_val = pickle_voting_clf.predict(X_val)\n",
    "#pickle_voting_pred_test = pickle_voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnr4nCQzIlOz"
   },
   "outputs": [],
   "source": [
    "named_estimators = [ # for each of the individual models\n",
    "    (\"SVM\", svm_pipeline),\n",
    "    (\"Logistic Regression\", lr_pipeline),\n",
    "    (\"XGBoost\", xgb_pipeline),\n",
    "    (\"AdaBoost\", ada_pipeline)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SrkWvL_MJ9o"
   },
   "outputs": [],
   "source": [
    "# voting classifier\n",
    "voting_clf = VotingClassifier(named_estimators, voting = \"soft\") #soft voting (predicts the class label based on the argmax of the sums of the predicted probabilities)\n",
    "\n",
    "# fit model\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "voting_pred_val = voting_clf.predict(X_val) # validation set \n",
    "voting_pred_test = voting_clf.predict(X_test) # test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickled Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1812,
     "status": "ok",
     "timestamp": 1651516590506,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "SFUC6NTCA1kS",
    "outputId": "f85ca20b-803c-4796-a52b-020cad16947b"
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "##mount drive to save data\n",
    "#from google.colab import drive\n",
    "#drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "error",
     "timestamp": 1651515959403,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "96rEtjBPA1kS",
    "outputId": "ed6d0ac7-5059-400d-97a4-2fd5abf02233"
   },
   "outputs": [],
   "source": [
    "##save SVM\n",
    "#pkl_filename = \"pickle_svm_clf.pkl\"\n",
    "#with open(pkl_filename, 'wb') as file:\n",
    "#    pickle.dump(svm_clf, file)\n",
    "#\n",
    "## Load from file\n",
    "##with open(pkl_filename, 'rb') as file:\n",
    "##    pickle_svm_clf = pickle.load(file)\n",
    "#\n",
    "##use pickle_svm_clf as model for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1651516593091,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "93n4GkxrA1kS",
    "outputId": "7cfb933f-95f4-49c4-a7ef-7321078d4a69"
   },
   "outputs": [],
   "source": [
    "## Load from file\n",
    "#pkl_filename = \"pickle_lg_clf.pkl\"\n",
    "#with open(pkl_filename, 'rb') as file:\n",
    "#    pickle_lg_clf = pickle.load(file)\n",
    "#\n",
    "##use pickle_svm_clf as model for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNkNNu7fA1kS"
   },
   "outputs": [],
   "source": [
    "##save LG\n",
    "#pkl_filename = \"pickle_lg_clf.pkl\"\n",
    "#with open(pkl_filename, 'wb') as file:\n",
    "#    pickle.dump(lr_clf, file)\n",
    "#\n",
    "## Load from file\n",
    "##with open(pkl_filename, 'rb') as file:\n",
    "##    pickle_svm_clf = pickle.load(file)\n",
    "#\n",
    "##use pickle_svm_clf as model for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "error",
     "timestamp": 1651516083313,
     "user": {
      "displayName": "Hannah Schweren",
      "userId": "17418859482307087546"
     },
     "user_tz": -120
    },
    "id": "0K6VAQNQA1kT",
    "outputId": "b9f21db8-7a1f-4a27-800b-bc6f2b0b91e3"
   },
   "outputs": [],
   "source": [
    "##save XGB\n",
    "#pkl_filename = \"pickle_xgb_clf.pkl\"\n",
    "#with open(pkl_filename, 'wb') as file:\n",
    "#    pickle.dump(svm_clf, file)\n",
    "#\n",
    "## Load from file\n",
    "##with open(pkl_filename, 'rb') as file:\n",
    "##    pickle_svm_clf = pickle.load(file)\n",
    "#\n",
    "##use pickle_svm_clf as model for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj2NRJTzA1kT"
   },
   "outputs": [],
   "source": [
    "##save svm\n",
    "#pkl_filename = \"pickle_svm_clf.pkl\"\n",
    "#with open(pkl_filename, 'wb') as file:\n",
    "#    pickle.dump(svm_clf, file)\n",
    "#\n",
    "## Load from file\n",
    "#with open(pkl_filename, 'rb') as file:\n",
    "#    pickle_svm_clf = pickle.load(file)\n",
    "#\n",
    "##use pickle_svm_clf as model for unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gogyysMESVOy"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qio2c88rMKT1"
   },
   "source": [
    "## #1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn0tNQw5QAmO"
   },
   "outputs": [],
   "source": [
    "# validation set\n",
    "# display results\n",
    "print_metrics(svm_pred_val,y_val)\n",
    "plot_confusion_matrix(confusion_matrix(y_val,svm_pred_val),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of SVM on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "# display results\n",
    "print_metrics(svm_pred_test,y_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test,svm_pred_test),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of SVM on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_hM1sTVMQei"
   },
   "source": [
    "## #2 Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "lFnaDSXDUVJz",
    "outputId": "1cda23d9-29b3-4488-fa22-8a4657d08999"
   },
   "outputs": [],
   "source": [
    "# validation set\n",
    "# display results\n",
    "print_metrics(lr_pred_val,y_val)\n",
    "plot_confusion_matrix(confusion_matrix(y_val,lr_pred_val),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of LR on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set \n",
    "# display results\n",
    "print_metrics(lr_pred_test,y_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test,lr_pred_test),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of LR on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frfqzpeUMZni"
   },
   "source": [
    "## #3 Extreme Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0a8XHEMU8U8"
   },
   "outputs": [],
   "source": [
    "# validation set \n",
    "# display results\n",
    "print_metrics(xgb_pred_val, y_val)\n",
    "plot_confusion_matrix(confusion_matrix(y_val, xgb_pred_val),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of XGB on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "# display results\n",
    "print_metrics(xgb_pred_test, y_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test, xgb_pred_test),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of XGB on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZSeMLEDMfTL"
   },
   "source": [
    "## #4 Adaptive Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "BsKRB4AJiAWh",
    "outputId": "68e9cbf1-31be-4a4c-8bc0-69449b8855d8"
   },
   "outputs": [],
   "source": [
    "# validation set \n",
    "# display results\n",
    "print_metrics(ada_pred_val,y_val)\n",
    "plot_confusion_matrix(confusion_matrix(y_val,ada_pred_val),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of Ada on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set \n",
    "# display results\n",
    "print_metrics(ada_pred_test,y_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test,ada_pred_test),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of Ada on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1paYU83BNAnz"
   },
   "source": [
    "## #5 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWKteqEiRXFP"
   },
   "outputs": [],
   "source": [
    "# validation set \n",
    "# display results\n",
    "print_metrics(voting_pred_val,y_val)\n",
    "plot_confusion_matrix(confusion_matrix(y_val,voting_pred_val),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of Ensemble on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ74LuujA4Ou"
   },
   "outputs": [],
   "source": [
    "# test set \n",
    "# display results\n",
    "print_metrics(voting_pred_test,y_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test,voting_pred_test),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of Ensemble on test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO60WIMcMiNO"
   },
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqTnVTIeBadM"
   },
   "outputs": [],
   "source": [
    "# create a df of misclassified posts\n",
    "svm_val_misclass_df = X_val[svm_pred_val!=y_val]\n",
    "\n",
    "# inspect df\n",
    "svm_val_misclass_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHONng5cDZRS"
   },
   "outputs": [],
   "source": [
    "#error analysis, to compair the false classifications of the different models\n",
    "\n",
    "false_pred_svm = val[(val[\"label\"] != svm_pred)]\n",
    "false_pred_gb = val[(val[\"label\"] != gb_pred)]\n",
    "false_pred_lr = val[(val[\"label\"] != lr_pred)]\n",
    "false_pred_xg = val[(val[\"label\"] != pred)]\n",
    "false_pred_ada = val[(val[\"label\"] != ada_pred)]\n",
    "false_pred_ensemble = val[(val[\"label\"] != voting_pred)]\n",
    "\n",
    "#common_mistakes = false_pred_svm = false_pred_gb = false_pred_lr = false_pred_xg = false_pred_ada = false_pred_ensemble\n",
    "print(false_pred_ensemble)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ5GX9khLilA"
   },
   "outputs": [],
   "source": [
    "#print(common_mistakes)\n",
    "common_mistakes = pd.merge(false_pred_svm, false_pred_ada, on=['tweet'], how='inner')\n",
    "print(common_mistakes)\n",
    "common_mistakes.to_csv('/content/drive/MyDrive/Final Project/COVID-fake-news-detection/model/error_analysis/common_mistakes.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-a1QA9WM1sP"
   },
   "outputs": [],
   "source": [
    "#mount drive to save data\n",
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fine_tuned_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
